{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as /teamspace/studios/this_studio/fine_tuning_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load Excel file\n",
    "file_path = \"/teamspace/studios/this_studio/Copy of evaluation_llama_by_themes_100(1).xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure column names are correct\n",
    "df = df[['Image Name', \"Caption\", 'Theme', 'Generated Script']]  # Adjust if necessary\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save as JSON\n",
    "json_file_path = \"/teamspace/studios/this_studio/fine_tuning_data.json\"\n",
    "with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Dataset saved as {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f9fccdc59d410ba705634dd5b5c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_32194/3820692361.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,587,520 || all params: 3,217,337,344 || trainable%: 0.1426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77/77 30:53, Epoch 9/11]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.219500</td>\n",
       "      <td>1.020484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.986873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.109000</td>\n",
       "      <td>0.963824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.078600</td>\n",
       "      <td>0.944292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.078600</td>\n",
       "      <td>0.927648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.059800</td>\n",
       "      <td>0.916031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.024200</td>\n",
       "      <td>0.908863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.048800</td>\n",
       "      <td>0.904244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=77, training_loss=1.0938526376501305, metrics={'train_runtime': 1879.6133, 'train_samples_per_second': 2.634, 'train_steps_per_second': 0.041, 'total_flos': 3.92347559380992e+16, 'train_loss': 1.0938526376501305, 'epoch': 9.70796460176991})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import copy\n",
    "\n",
    "# ðŸš€ Enable memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load JSON dataset\n",
    "json_path = \"/teamspace/studios/this_studio/fine_tuning_data.json\"\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=json_path, split=\"train[:90%]\"),\n",
    "    \"validation\": load_dataset(\"json\", data_files=json_path, split=\"train[90%:]\")\n",
    "})\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = [f\"Caption: {c} Theme: {t} Script: {s}\" for c, t, s in zip(examples[\"Caption\"], examples[\"Theme\"], examples[\"Generated Script\"])]\n",
    "    tokenized = tokenizer(inputs, truncation=False, padding=\"longest\")\n",
    "    tokenized[\"labels\"] = copy.deepcopy(tokenized[\"input_ids\"])\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"Image Name\", \"Caption\", \"Theme\", \"Generated Script\"])\n",
    "\n",
    "# Save tokenized datasets\n",
    "torch.save(tokenized_datasets[\"train\"], \"train_dataset.pt\")\n",
    "torch.save(tokenized_datasets[\"validation\"], \"val_dataset.pt\")\n",
    "\n",
    "# Reload tokenized dataset\n",
    "train_dataset = torch.load(\"train_dataset.pt\")\n",
    "val_dataset = torch.load(\"val_dataset.pt\")\n",
    "\n",
    "# ðŸš€ 4-bit Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# ðŸš€ Load Quantized Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ðŸš€ Prepare Model for LoRA Training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ðŸš€ LoRA Configuration (Train Small Adapter Layers)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of LoRA adapters\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for stability\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# ðŸš€ Apply LoRA to Model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# ðŸš€ Training arguments (Optimized for LoRA + 4-bit)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,  # âœ… LoRA reduces memory, so batch size can increase\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=11,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # âœ… FP16 speeds up training\n",
    "    gradient_checkpointing=True,\n",
    "    torch_compile=False,  # âœ… Fix InternalTorchDynamoError\n",
    ")\n",
    "\n",
    "# Free up GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ðŸš€ Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to /teamspace/studios/this_studio/results\n"
     ]
    }
   ],
   "source": [
    "# Define save directory\n",
    "save_directory = \"/teamspace/studios/this_studio/results\"\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b63cd3ebe954a0a9903b738fba34687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Generating script...\n",
      "Script generation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = \"/teamspace/studios/this_studio/results\"\n",
    "DATASET_PATH = \"/teamspace/studios/this_studio/evaluation_llama_by_themes_100.xlsx\"\n",
    "OUTPUT_PATH = \"/teamspace/studios/this_studio/generated/generated_results.xlsx\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "print(\"Loading model...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Function to generate scripts without limitations\n",
    "def generate_script(caption, theme):\n",
    "    input_text = f\"Caption: {caption}\\nTheme: {theme}\\nScript:\\n\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    print(\"Generating script...\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            max_length=4096,  # Large max length to ensure full generation\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    script_start = generated_text.find(\"Script:\")\n",
    "    if script_start != -1:\n",
    "        generated_script = generated_text[script_start + len(\"Script:\"):].strip()\n",
    "    else:\n",
    "        generated_script = generated_text.strip()\n",
    "\n",
    "    return generated_script, response_time\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(DATASET_PATH)\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Generate scripts for all samples\n",
    "for index, row in df.iterrows():\n",
    "    caption = row[\"Caption\"]\n",
    "    theme = row[\"Theme\"]\n",
    "\n",
    "    generated_script, response_time = generate_script(caption, theme)\n",
    "\n",
    "    df.at[index, \"Generated Script\"] = generated_script\n",
    "    df.at[index, \"Response Time\"] = response_time\n",
    "\n",
    "# Save results to Excel\n",
    "df.to_excel(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"Script generation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4130cfc6daa541558cc3cad53f94e331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== AVERAGE METRICS =====\n",
      "Average BLEU Score: 0.1542\n",
      "Average Semantic Similarity: 0.7771\n",
      "Average Accuracy: 0.6525\n",
      "Average Perplexity: 2.4889\n",
      "Average Response Time (s): 16.2328\n",
      "Results saved to: /teamspace/studios/this_studio/evaluated_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the Excel file\n",
    "data_path = \"/teamspace/studios/this_studio/generated/generated_results.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "\n",
    "df.fillna(\"\", inplace=True)  # Replace NaN values with empty strings\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load a base Llama model for perplexity calculation\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n",
    "\n",
    "def compute_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs, labels=inputs)\n",
    "    loss = outputs.loss.item()\n",
    "    return math.exp(loss)\n",
    "\n",
    "def compute_semantic_similarity(text1, text2):\n",
    "    embeddings1 = semantic_model.encode(text1, convert_to_tensor=True)\n",
    "    embeddings2 = semantic_model.encode(text2, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    return similarity.item()\n",
    "\n",
    "# Initialize total scores\n",
    "total_bleu, total_similarity, total_accuracy, total_ppl, total_response_time = 0, 0, 0, 0, 0\n",
    "num_samples = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    untrained_script = row[\"Untrained Llama Script\"].strip()\n",
    "    generated_script = row[\"Generated Script\"].strip()\n",
    "    response_time = row[\"Response Time\"]\n",
    "    \n",
    "    # Compute BLEU Score if reference exists\n",
    "    if untrained_script:\n",
    "        bleu_score = bleu_metric.compute(predictions=[generated_script], references=[[untrained_script]])[\"bleu\"]\n",
    "        semantic_similarity = compute_semantic_similarity(generated_script, untrained_script)\n",
    "    else:\n",
    "        bleu_score, semantic_similarity = 0.0, 0.0\n",
    "    \n",
    "    # Compute Accuracy\n",
    "    accuracy = 0.2 * bleu_score + 0.8 * semantic_similarity\n",
    "    \n",
    "    # Compute Perplexity\n",
    "    ppl_score = compute_perplexity(generated_script)\n",
    "    \n",
    "    # Store results\n",
    "    df.at[index, \"BLEU Score\"] = bleu_score\n",
    "    df.at[index, \"Semantic Similarity\"] = semantic_similarity\n",
    "    df.at[index, \"Accuracy\"] = accuracy\n",
    "    df.at[index, \"Perplexity\"] = ppl_score\n",
    "    \n",
    "    # Accumulate scores\n",
    "    total_bleu += bleu_score\n",
    "    total_similarity += semantic_similarity\n",
    "    total_accuracy += accuracy\n",
    "    total_ppl += ppl_score\n",
    "    total_response_time += response_time\n",
    "\n",
    "# Compute averages\n",
    "if num_samples > 0:\n",
    "    avg_bleu = total_bleu / num_samples\n",
    "    avg_similarity = total_similarity / num_samples\n",
    "    avg_accuracy = total_accuracy / num_samples\n",
    "    avg_ppl = total_ppl / num_samples\n",
    "    avg_response_time = total_response_time / num_samples\n",
    "else:\n",
    "    avg_bleu = avg_similarity = avg_accuracy = avg_ppl = avg_response_time = 0.0\n",
    "\n",
    "print(\"\\n===== AVERAGE METRICS =====\")\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "print(f\"Average Semantic Similarity: {avg_similarity:.4f}\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Perplexity: {avg_ppl:.4f}\")\n",
    "print(f\"Average Response Time (s): {avg_response_time:.4f}\")\n",
    "\n",
    "# Save results to Excel\n",
    "output_path = \"/teamspace/studios/this_studio/evaluated_results.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
